{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "664641bb",
   "metadata": {},
   "source": [
    "Sliding Window: https://doheon.github.io/%EC%BD%94%EB%93%9C%EA%B5%AC%ED%98%84/time-series/ci-3.lstm-post/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59183706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "from pykrx import stock\n",
    "\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error \n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c8b4eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager\n",
    "[f.name for f in matplotlib.font_manager.fontManager.ttflist if 'Nanum' in f.name]\n",
    "# 유니코드 깨짐 현상 해결\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "# 나눔 고딕 폰트 적용\n",
    "mpl.rcParams['font.family'] = 'NanumGothic'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884f3bef",
   "metadata": {},
   "source": [
    "# 1. 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5b1f58",
   "metadata": {},
   "source": [
    "## 슬라이딩 윈도우 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b734940",
   "metadata": {},
   "outputs": [],
   "source": [
    "## window dataset을 생성하는 클래스\n",
    "class windowDataset(Dataset):\n",
    "    def __init__(self, y, input_window, output_window, stride):\n",
    "        #총 데이터의 개수\n",
    "        L = y.shape[0]\n",
    "        #stride씩 움직일 때 생기는 총 sample의 개수\n",
    "        num_samples = (L - input_window - output_window) // stride + 1\n",
    "\n",
    "        #input과 output : shape = (window 크기, sample 개수)\n",
    "        X = np.zeros([input_window*30, num_samples])\n",
    "        Y = np.zeros([output_window*30, num_samples])\n",
    "\n",
    "        for i in np.arange(num_samples):\n",
    "            start_x = stride*i\n",
    "            end_x = start_x + input_window\n",
    "            X[:,i] = y[start_x:end_x].reshape(1, -1).flatten()\n",
    "\n",
    "            start_y = stride*i + input_window\n",
    "            end_y = start_y + output_window\n",
    "            Y[:,i] = y[start_y:end_y].reshape(1, -1).flatten()\n",
    "\n",
    "        X = X.reshape(X.shape[0], X.shape[1], 1).transpose((1,0,2))\n",
    "        Y = Y.reshape(Y.shape[0], Y.shape[1], 1).transpose((1,0,2))\n",
    "        self.x = X\n",
    "        self.y = Y     \n",
    "        self.len = len(X)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1658186f",
   "metadata": {},
   "source": [
    "input window, output window, stride를 입력받고 iw + ow만큼의 길이를 stride 간격으로 sliding하면서 데이터셋을 생성  \n",
    "결과의 첫 번째 값으로는 input, 두 번째 값으로는 output이 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6431e584",
   "metadata": {},
   "source": [
    "## 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951dfc36",
   "metadata": {},
   "source": [
    "### 모델 함수 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9f47d8",
   "metadata": {},
   "source": [
    "* encoder: input을 통해 decoder에 전달할 hidden state 생성\n",
    "* decoder: input의 마지막 값과 encoder에서 받은 hidden state를 이용하여 한 개의 값을 예측\n",
    "* encoder decoder: 위의 두 모델을 합침. 원하는 길이의 아웃풋이 나올 때까지 decoder를 여러번 실행시켜서 최종 output을 생성. 원할한 학습을 위해 decoder의 input으로 실제 값을 넣는 teach forcing 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a84b6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "## input으로부터 입력을 받고 lstm을 이용하여 디코더에 전달할 hidden state 생성\n",
    "class lstm_encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers = 1):\n",
    "        super(lstm_encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        lstm_out, self.hidden = self.lstm(x_input)\n",
    "        return lstm_out, self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c01b8389",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sequence의 이전값 하나와, 이전 결과의 hidden state를 입력받아서 다음 값 하나를 예측\n",
    "## 마지막에 fc layer를 연결해서 input size와 동일하게 크기를 맞춰줌\n",
    "class lstm_decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers = 1):\n",
    "        super(lstm_decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, input_size)           \n",
    "\n",
    "    def forward(self, x_input, encoder_hidden_states):\n",
    "        lstm_out, self.hidden = self.lstm(x_input.unsqueeze(-1), encoder_hidden_states)\n",
    "        output = self.linear(lstm_out)\n",
    "        \n",
    "        return output, self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fc6e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 위의 두 모델 합치기\n",
    "## 인코더를 한 번 실행시키고 인코더에서 전달받은 hidden state와 input의 마지막 값을 decoder에 전달해서 다음 예측값을 구함\n",
    "## 여기서 나온 값과 hidden state를 반복적으로 사용해서 원하는 길이가 될 때까지 decoder 실행\n",
    "class lstm_encoder_decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(lstm_encoder_decoder, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.encoder = lstm_encoder(input_size = input_size, hidden_size = hidden_size)\n",
    "        self.decoder = lstm_decoder(input_size = input_size, hidden_size = hidden_size)\n",
    "\n",
    "    def forward(self, inputs, targets, target_len, teacher_forcing_ratio):\n",
    "        batch_size = inputs.shape[0]\n",
    "        input_size = inputs.shape[2]\n",
    "\n",
    "        outputs = torch.zeros(batch_size, target_len*30, input_size)\n",
    "\n",
    "        _, hidden = self.encoder(inputs)\n",
    "        decoder_input = inputs[:,-1, :]\n",
    "        \n",
    "        #원하는 길이가 될 때까지 decoder를 실행한다.\n",
    "        for t in range(target_len*30): \n",
    "            out, hidden = self.decoder(decoder_input, hidden)\n",
    "            out =  out.squeeze(1)\n",
    "            \n",
    "            # teacher forcing을 구현한다.\n",
    "            # teacher forcing에 해당하면 다음 인풋값으로는 예측한 값이 아니라 실제 값을 사용한다.\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                decoder_input = targets[:, t, :]\n",
    "            else:\n",
    "                decoder_input = out\n",
    "            outputs[:,t,:] = out\n",
    "        return outputs\n",
    "\n",
    "    # 편의성을 위해 예측해주는 함수도 생성한다.\n",
    "    def predict(self, inputs, target_len):\n",
    "        self.eval()\n",
    "        inputs = inputs.unsqueeze(0)\n",
    "        batch_size = inputs.shape[0]\n",
    "        input_size = inputs.shape[2]\n",
    "        outputs = torch.zeros(batch_size, target_len*30, input_size)\n",
    "        _, hidden = self.encoder(inputs)\n",
    "        decoder_input = inputs[:,-1, :]\n",
    "        for t in range(target_len*30): \n",
    "            out, hidden = self.decoder(decoder_input, hidden)\n",
    "            out =  out.squeeze(1)\n",
    "            decoder_input = out\n",
    "            outputs[:,t,:] = out\n",
    "        return outputs.detach().numpy()[0,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e0456f",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c158b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93bc2a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, ow):\n",
    "    model = lstm_encoder_decoder(input_size=1, hidden_size=16).to(device)\n",
    "    \n",
    "    learning_rate=0.01\n",
    "    epoch = 3000\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    model.train()\n",
    "    loss_avgs = []\n",
    "    with tqdm(range(epoch)) as tr:\n",
    "        for i in tr:\n",
    "            total_loss = 0.0\n",
    "            tf_ratio = max(0.6 - i*0.0001, 0)  # 0.6에서 teacher forcing을 점진적으로 줄여나감\n",
    "#             tf_ratio = 0.6  # tf 조정 없이 0.6으로 통일할 때 사용\n",
    "            for x,y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                x = x.to(device).float()\n",
    "                y = y.to(device).float()\n",
    "                output = model(x, y, ow, tf_ratio).to(device)\n",
    "                loss = criterion(output, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.cpu().item()\n",
    "            tr.set_postfix(loss=\"{0:.5f}\".format(total_loss/len(train_loader)))\n",
    "            loss_avgs.append(total_loss/len(train_loader))\n",
    "    return model, loss_avgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c1b34",
   "metadata": {},
   "source": [
    "## 최종 예측 모델 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "724e9437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecasting_model(df, iw, ow, st):\n",
    "    print('iw='+str(iw), 'ow='+str(ow), 'st='+str(st))\n",
    "\n",
    "    ## 데이터 스케일링\n",
    "    ## 데이터 분할\n",
    "    # 마지막 일주일의 데이터를 예측하는 것이 목표\n",
    "    scaler = MinMaxScaler()\n",
    "    train_data = np.array(df)\n",
    "    scaler.fit(train_data)\n",
    "    train_np = scaler.transform(train_data)\n",
    "\n",
    "    ## 슬라이딩 윈도우 설정\n",
    "    df_train_dataset = windowDataset(train_np, input_window=iw, output_window=ow, stride=st)\n",
    "    df_train_loader = DataLoader(df_train_dataset, batch_size=256)\n",
    "\n",
    "    ## 모델 학습\n",
    "    df_model, loss_avg = train(df_train_loader, ow)\n",
    "\n",
    "#     ## 모델 저장\n",
    "#     joblib.dump(df_model, './result/final/model/out'+str(ow)+'/stride'+str(st)+'/'+str(iw)+'days_model.pkl')\n",
    "\n",
    "#     ## 예측\n",
    "#     device = torch.device('cpu')\n",
    "#     df_predict = df_model.predict(torch.tensor(train_np).reshape(-1,1).to(device).float(), target_len=ow)\n",
    "    \n",
    "#     ## 예측값 저장\n",
    "#     standard = pd.to_datetime('2023-01-01')\n",
    "#     df_predict_df = pd.DataFrame(scaler.inverse_transform(df_predict.reshape(ow, 30)), columns=company, index=pd.date_range(standard, standard+timedelta(days=ow-1)))\n",
    "#     df_predict_df.to_csv('./result/final/predict/out'+str(ow)+'/stride'+str(st)+'/'+str(iw)+'days.csv', encoding='cp949')\n",
    "    return df_model, loss_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395df9ea",
   "metadata": {},
   "source": [
    "# 2. 데이터 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7da05c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = pd.read_csv('./source/30/stock_top30.csv', encoding='cp949')\n",
    "stocks['Date'] = pd.to_datetime(stocks['Date'])\n",
    "stocks.set_index(['Date'], inplace=True)\n",
    "stocks = stocks.loc[:'2022-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca7bf1ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>삼성전자</th>\n",
       "      <th>SK하이닉스</th>\n",
       "      <th>LG화학</th>\n",
       "      <th>삼성전자우</th>\n",
       "      <th>삼성SDI</th>\n",
       "      <th>현대차</th>\n",
       "      <th>NAVER</th>\n",
       "      <th>기아</th>\n",
       "      <th>카카오</th>\n",
       "      <th>POSCO홀딩스</th>\n",
       "      <th>...</th>\n",
       "      <th>KT&amp;G</th>\n",
       "      <th>하나금융지주</th>\n",
       "      <th>LG</th>\n",
       "      <th>고려아연</th>\n",
       "      <th>SK텔레콤</th>\n",
       "      <th>삼성전기</th>\n",
       "      <th>두산에너빌리티</th>\n",
       "      <th>엔씨소프트</th>\n",
       "      <th>HMM</th>\n",
       "      <th>S-Oil</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>31520.000000</td>\n",
       "      <td>26600.0</td>\n",
       "      <td>340000.000000</td>\n",
       "      <td>17220.000000</td>\n",
       "      <td>152500.000000</td>\n",
       "      <td>216000.000000</td>\n",
       "      <td>71413.000000</td>\n",
       "      <td>56300.000000</td>\n",
       "      <td>18801.0</td>\n",
       "      <td>360500.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>78700.000000</td>\n",
       "      <td>36300.000000</td>\n",
       "      <td>63292.0</td>\n",
       "      <td>405000.000000</td>\n",
       "      <td>25967.0</td>\n",
       "      <td>101000.0</td>\n",
       "      <td>35077.000000</td>\n",
       "      <td>154000.0</td>\n",
       "      <td>144704.000000</td>\n",
       "      <td>105500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>30860.000000</td>\n",
       "      <td>26650.0</td>\n",
       "      <td>342500.000000</td>\n",
       "      <td>17200.000000</td>\n",
       "      <td>155500.000000</td>\n",
       "      <td>206000.000000</td>\n",
       "      <td>69373.000000</td>\n",
       "      <td>54600.000000</td>\n",
       "      <td>18881.0</td>\n",
       "      <td>370000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>77000.000000</td>\n",
       "      <td>36700.000000</td>\n",
       "      <td>64048.0</td>\n",
       "      <td>406500.000000</td>\n",
       "      <td>25880.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>37372.000000</td>\n",
       "      <td>155000.0</td>\n",
       "      <td>137183.000000</td>\n",
       "      <td>105500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>30500.000000</td>\n",
       "      <td>26350.0</td>\n",
       "      <td>331000.000000</td>\n",
       "      <td>16920.000000</td>\n",
       "      <td>157000.000000</td>\n",
       "      <td>206000.000000</td>\n",
       "      <td>71099.000000</td>\n",
       "      <td>53600.000000</td>\n",
       "      <td>20131.0</td>\n",
       "      <td>367000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>76200.000000</td>\n",
       "      <td>36550.000000</td>\n",
       "      <td>63481.0</td>\n",
       "      <td>378000.000000</td>\n",
       "      <td>26484.0</td>\n",
       "      <td>99000.0</td>\n",
       "      <td>37564.000000</td>\n",
       "      <td>158500.0</td>\n",
       "      <td>135679.000000</td>\n",
       "      <td>105000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-05</th>\n",
       "      <td>30466.666667</td>\n",
       "      <td>26200.0</td>\n",
       "      <td>329166.666667</td>\n",
       "      <td>17013.333333</td>\n",
       "      <td>155833.333333</td>\n",
       "      <td>206833.333333</td>\n",
       "      <td>72511.666667</td>\n",
       "      <td>53733.333333</td>\n",
       "      <td>20386.0</td>\n",
       "      <td>367666.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>76733.333333</td>\n",
       "      <td>36633.333333</td>\n",
       "      <td>63607.0</td>\n",
       "      <td>379833.333333</td>\n",
       "      <td>26628.0</td>\n",
       "      <td>99500.0</td>\n",
       "      <td>37589.333333</td>\n",
       "      <td>158000.0</td>\n",
       "      <td>135779.333333</td>\n",
       "      <td>104166.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-06</th>\n",
       "      <td>30433.333333</td>\n",
       "      <td>26050.0</td>\n",
       "      <td>327333.333333</td>\n",
       "      <td>17106.666667</td>\n",
       "      <td>154666.666667</td>\n",
       "      <td>207666.666667</td>\n",
       "      <td>73924.333333</td>\n",
       "      <td>53866.666667</td>\n",
       "      <td>20641.0</td>\n",
       "      <td>368333.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>77266.666667</td>\n",
       "      <td>36716.666667</td>\n",
       "      <td>63733.0</td>\n",
       "      <td>381666.666667</td>\n",
       "      <td>26772.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>37614.666667</td>\n",
       "      <td>157500.0</td>\n",
       "      <td>135879.666667</td>\n",
       "      <td>103333.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>58100.000000</td>\n",
       "      <td>77000.0</td>\n",
       "      <td>628000.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>624000.000000</td>\n",
       "      <td>158000.000000</td>\n",
       "      <td>181000.000000</td>\n",
       "      <td>64100.000000</td>\n",
       "      <td>54400.0</td>\n",
       "      <td>292000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>95100.000000</td>\n",
       "      <td>45300.000000</td>\n",
       "      <td>81900.0</td>\n",
       "      <td>565000.000000</td>\n",
       "      <td>49300.0</td>\n",
       "      <td>133000.0</td>\n",
       "      <td>14850.000000</td>\n",
       "      <td>432000.0</td>\n",
       "      <td>21850.000000</td>\n",
       "      <td>89100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-28</th>\n",
       "      <td>56600.000000</td>\n",
       "      <td>76000.0</td>\n",
       "      <td>602000.000000</td>\n",
       "      <td>51300.000000</td>\n",
       "      <td>603000.000000</td>\n",
       "      <td>154000.000000</td>\n",
       "      <td>180000.000000</td>\n",
       "      <td>60900.000000</td>\n",
       "      <td>53600.0</td>\n",
       "      <td>291000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>91400.000000</td>\n",
       "      <td>42150.000000</td>\n",
       "      <td>79500.0</td>\n",
       "      <td>575000.000000</td>\n",
       "      <td>48200.0</td>\n",
       "      <td>134000.0</td>\n",
       "      <td>15700.000000</td>\n",
       "      <td>433500.0</td>\n",
       "      <td>20100.000000</td>\n",
       "      <td>86300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-29</th>\n",
       "      <td>55300.000000</td>\n",
       "      <td>75000.0</td>\n",
       "      <td>600000.000000</td>\n",
       "      <td>50500.000000</td>\n",
       "      <td>591000.000000</td>\n",
       "      <td>151000.000000</td>\n",
       "      <td>177500.000000</td>\n",
       "      <td>59300.000000</td>\n",
       "      <td>53100.0</td>\n",
       "      <td>276500.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>91500.000000</td>\n",
       "      <td>42050.000000</td>\n",
       "      <td>78100.0</td>\n",
       "      <td>564000.000000</td>\n",
       "      <td>47400.0</td>\n",
       "      <td>130500.0</td>\n",
       "      <td>15400.000000</td>\n",
       "      <td>448000.0</td>\n",
       "      <td>19550.000000</td>\n",
       "      <td>83400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30</th>\n",
       "      <td>55350.000000</td>\n",
       "      <td>75175.0</td>\n",
       "      <td>601000.000000</td>\n",
       "      <td>50575.000000</td>\n",
       "      <td>593750.000000</td>\n",
       "      <td>152500.000000</td>\n",
       "      <td>178000.000000</td>\n",
       "      <td>59850.000000</td>\n",
       "      <td>53000.0</td>\n",
       "      <td>275375.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>90875.000000</td>\n",
       "      <td>41737.500000</td>\n",
       "      <td>77725.0</td>\n",
       "      <td>557750.000000</td>\n",
       "      <td>47362.5</td>\n",
       "      <td>131000.0</td>\n",
       "      <td>15362.500000</td>\n",
       "      <td>443875.0</td>\n",
       "      <td>19500.000000</td>\n",
       "      <td>83250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31</th>\n",
       "      <td>55400.000000</td>\n",
       "      <td>75350.0</td>\n",
       "      <td>602000.000000</td>\n",
       "      <td>50650.000000</td>\n",
       "      <td>596500.000000</td>\n",
       "      <td>154000.000000</td>\n",
       "      <td>178500.000000</td>\n",
       "      <td>60400.000000</td>\n",
       "      <td>52900.0</td>\n",
       "      <td>274250.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>90250.000000</td>\n",
       "      <td>41425.000000</td>\n",
       "      <td>77350.0</td>\n",
       "      <td>551500.000000</td>\n",
       "      <td>47325.0</td>\n",
       "      <td>131500.0</td>\n",
       "      <td>15325.000000</td>\n",
       "      <td>439750.0</td>\n",
       "      <td>19450.000000</td>\n",
       "      <td>83100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3651 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    삼성전자   SK하이닉스           LG화학         삼성전자우          삼성SDI  \\\n",
       "Date                                                                            \n",
       "2013-01-02  31520.000000  26600.0  340000.000000  17220.000000  152500.000000   \n",
       "2013-01-03  30860.000000  26650.0  342500.000000  17200.000000  155500.000000   \n",
       "2013-01-04  30500.000000  26350.0  331000.000000  16920.000000  157000.000000   \n",
       "2013-01-05  30466.666667  26200.0  329166.666667  17013.333333  155833.333333   \n",
       "2013-01-06  30433.333333  26050.0  327333.333333  17106.666667  154666.666667   \n",
       "...                  ...      ...            ...           ...            ...   \n",
       "2022-12-27  58100.000000  77000.0  628000.000000  52500.000000  624000.000000   \n",
       "2022-12-28  56600.000000  76000.0  602000.000000  51300.000000  603000.000000   \n",
       "2022-12-29  55300.000000  75000.0  600000.000000  50500.000000  591000.000000   \n",
       "2022-12-30  55350.000000  75175.0  601000.000000  50575.000000  593750.000000   \n",
       "2022-12-31  55400.000000  75350.0  602000.000000  50650.000000  596500.000000   \n",
       "\n",
       "                      현대차          NAVER            기아      카카오  \\\n",
       "Date                                                              \n",
       "2013-01-02  216000.000000   71413.000000  56300.000000  18801.0   \n",
       "2013-01-03  206000.000000   69373.000000  54600.000000  18881.0   \n",
       "2013-01-04  206000.000000   71099.000000  53600.000000  20131.0   \n",
       "2013-01-05  206833.333333   72511.666667  53733.333333  20386.0   \n",
       "2013-01-06  207666.666667   73924.333333  53866.666667  20641.0   \n",
       "...                   ...            ...           ...      ...   \n",
       "2022-12-27  158000.000000  181000.000000  64100.000000  54400.0   \n",
       "2022-12-28  154000.000000  180000.000000  60900.000000  53600.0   \n",
       "2022-12-29  151000.000000  177500.000000  59300.000000  53100.0   \n",
       "2022-12-30  152500.000000  178000.000000  59850.000000  53000.0   \n",
       "2022-12-31  154000.000000  178500.000000  60400.000000  52900.0   \n",
       "\n",
       "                 POSCO홀딩스  ...          KT&G        하나금융지주       LG  \\\n",
       "Date                       ...                                        \n",
       "2013-01-02  360500.000000  ...  78700.000000  36300.000000  63292.0   \n",
       "2013-01-03  370000.000000  ...  77000.000000  36700.000000  64048.0   \n",
       "2013-01-04  367000.000000  ...  76200.000000  36550.000000  63481.0   \n",
       "2013-01-05  367666.666667  ...  76733.333333  36633.333333  63607.0   \n",
       "2013-01-06  368333.333333  ...  77266.666667  36716.666667  63733.0   \n",
       "...                   ...  ...           ...           ...      ...   \n",
       "2022-12-27  292000.000000  ...  95100.000000  45300.000000  81900.0   \n",
       "2022-12-28  291000.000000  ...  91400.000000  42150.000000  79500.0   \n",
       "2022-12-29  276500.000000  ...  91500.000000  42050.000000  78100.0   \n",
       "2022-12-30  275375.000000  ...  90875.000000  41737.500000  77725.0   \n",
       "2022-12-31  274250.000000  ...  90250.000000  41425.000000  77350.0   \n",
       "\n",
       "                     고려아연    SK텔레콤      삼성전기       두산에너빌리티     엔씨소프트  \\\n",
       "Date                                                                   \n",
       "2013-01-02  405000.000000  25967.0  101000.0  35077.000000  154000.0   \n",
       "2013-01-03  406500.000000  25880.0  100000.0  37372.000000  155000.0   \n",
       "2013-01-04  378000.000000  26484.0   99000.0  37564.000000  158500.0   \n",
       "2013-01-05  379833.333333  26628.0   99500.0  37589.333333  158000.0   \n",
       "2013-01-06  381666.666667  26772.0  100000.0  37614.666667  157500.0   \n",
       "...                   ...      ...       ...           ...       ...   \n",
       "2022-12-27  565000.000000  49300.0  133000.0  14850.000000  432000.0   \n",
       "2022-12-28  575000.000000  48200.0  134000.0  15700.000000  433500.0   \n",
       "2022-12-29  564000.000000  47400.0  130500.0  15400.000000  448000.0   \n",
       "2022-12-30  557750.000000  47362.5  131000.0  15362.500000  443875.0   \n",
       "2022-12-31  551500.000000  47325.0  131500.0  15325.000000  439750.0   \n",
       "\n",
       "                      HMM          S-Oil  \n",
       "Date                                      \n",
       "2013-01-02  144704.000000  105500.000000  \n",
       "2013-01-03  137183.000000  105500.000000  \n",
       "2013-01-04  135679.000000  105000.000000  \n",
       "2013-01-05  135779.333333  104166.666667  \n",
       "2013-01-06  135879.666667  103333.333333  \n",
       "...                   ...            ...  \n",
       "2022-12-27   21850.000000   89100.000000  \n",
       "2022-12-28   20100.000000   86300.000000  \n",
       "2022-12-29   19550.000000   83400.000000  \n",
       "2022-12-30   19500.000000   83250.000000  \n",
       "2022-12-31   19450.000000   83100.000000  \n",
       "\n",
       "[3651 rows x 30 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4314aa4b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iw=15 ow=7 st=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 3000/3000 [5:01:59<00:00,  6.04s/it, loss=0.78990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iw=15 ow=7 st=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 3000/3000 [2:47:29<00:00,  3.35s/it, loss=0.04144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iw=15 ow=7 st=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 3000/3000 [1:45:47<00:00,  2.12s/it, loss=0.04481]\n"
     ]
    }
   ],
   "source": [
    "## df, iw, ow, st, name\n",
    "iw = 15\n",
    "ow = 7\n",
    "# for ow in [4, 7, 10, 15, 30]:  # output\n",
    "for st in [1, 2, 3]:  # stride\n",
    "    tmp_model, loss_list = forecasting_model(stocks, iw, ow, st)\n",
    "    joblib.dump(tmp_model, './result/final2/iw'+str(iw)+'_ow'+str(ow)+'_st'+str(st)+'_model.pkl')\n",
    "    pd.DataFrame(loss_list, columns=['loss']).to_csv('./result/final2/iw'+str(iw)+'_ow'+str(ow)+'_st'+str(st)+'_loss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f19ef9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iw=15 ow=10 st=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 3000/3000 [6:43:35<00:00,  8.07s/it, loss=0.39409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iw=15 ow=10 st=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 3000/3000 [3:28:50<00:00,  4.18s/it, loss=0.03918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iw=15 ow=10 st=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 3000/3000 [2:24:48<00:00,  2.90s/it, loss=0.04366]\n"
     ]
    }
   ],
   "source": [
    "## df, iw, ow, st, name\n",
    "iw = 15\n",
    "ow = 10\n",
    "# for ow in [4, 7, 10, 15, 30]:  # output\n",
    "for st in [1, 2, 3]:  # stride\n",
    "    tmp_model, loss_list = forecasting_model(stocks, iw, ow, st)\n",
    "    joblib.dump(tmp_model, './result/final2/iw'+str(iw)+'_ow'+str(ow)+'_st'+str(st)+'_model.pkl')\n",
    "    pd.DataFrame(loss_list, columns=['loss']).to_csv('./result/final2/iw'+str(iw)+'_ow'+str(ow)+'_st'+str(st)+'_loss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb6fb0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iw=30 ow=30 st=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                | 2/3000 [01:56<48:34:49, 58.34s/it, loss=0.06636]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# for ow in [4, 7, 10, 15, 30]:  # output\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m st \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]:  \u001b[38;5;66;03m# stride\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     tmp_model, loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mforecasting_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     joblib\u001b[38;5;241m.\u001b[39mdump(tmp_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./result/final2/top30/iw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(iw)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_ow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(ow)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_st\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(st)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m     pd\u001b[38;5;241m.\u001b[39mDataFrame(loss_list, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./result/final2/top30/iw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(iw)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_ow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(ow)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_st\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(st)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_loss.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mforecasting_model\u001b[1;34m(df, iw, ow, st)\u001b[0m\n\u001b[0;32m     14\u001b[0m     df_train_loader \u001b[38;5;241m=\u001b[39m DataLoader(df_train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m## 모델 학습\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     df_model, loss_avg \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#     ## 모델 저장\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#     joblib.dump(df_model, './result/final/model/out'+str(ow)+'/stride'+str(st)+'/'+str(iw)+'days_model.pkl')\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#     df_predict_df = pd.DataFrame(scaler.inverse_transform(df_predict.reshape(ow, 30)), columns=company, index=pd.date_range(standard, standard+timedelta(days=ow-1)))\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#     df_predict_df.to_csv('./result/final/predict/out'+str(ow)+'/stride'+str(st)+'/'+str(iw)+'days.csv', encoding='cp949')\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_model, loss_avg\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, ow)\u001b[0m\n\u001b[0;32m     20\u001b[0m output \u001b[38;5;241m=\u001b[39m model(x, y, ow, tf_ratio)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, y)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     24\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## df, iw, ow, st, name\n",
    "iw = 30\n",
    "ow = 30\n",
    "# for ow in [4, 7, 10, 15, 30]:  # output\n",
    "for st in [1, 2, 3]:  # stride\n",
    "    tmp_model, loss_list = forecasting_model(stocks, iw, ow, st)\n",
    "    joblib.dump(tmp_model, './result/final2/top30/iw'+str(iw)+'_ow'+str(ow)+'_st'+str(st)+'_model.pkl')\n",
    "    pd.DataFrame(loss_list, columns=['loss']).to_csv('./result/final2/top30/iw'+str(iw)+'_ow'+str(ow)+'_st'+str(st)+'_loss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5961d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
